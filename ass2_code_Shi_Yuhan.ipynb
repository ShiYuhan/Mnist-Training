{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "PcJcP0eTRB1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Code block catalog\n",
        "\n",
        "1.   28\\*28 to 7\\*7\n",
        "2.   plot\n",
        "3.   tensorflow_hidden_layers*2\n",
        "4.   tensorflow_hidden_layers*1\n",
        "5.   tensorflow_hidden_layers*3\n",
        "6.   sklearn\n"
      ]
    },
    {
      "metadata": {
        "id": "vHCiZUHvxa49",
        "colab_type": "code",
        "outputId": "f9b8ebab-51a4-48c2-b3b1-6e62b7c363aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#28*28 to 7*7\n",
        "\n",
        "#下载mnist数据集\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "# x_train=x_train.reshape(60000,784)\n",
        "# x_test=x_test.reshape(10000,784)\n",
        "\n",
        "# ----------------28*28 to 7*7----------------\n",
        "import numpy\n",
        "x_train1= numpy.zeros(shape=(60000,49))\n",
        "x_test1=numpy.zeros(shape=(10000,49))\n",
        "i=0\n",
        "for i in range(60000):\n",
        "    j=0\n",
        "    m=0\n",
        "    n=0\n",
        "#     print ('i=',i)\n",
        "    for m in range (0,27,4):\n",
        "        n=0\n",
        "#         print('m=',m)\n",
        "        for  n in range (0,27,4):\n",
        "            x_train1[i][j]=(x_train[i][m][n]+x_train[i][m][n+1]+x_train[i][m][n+2]+x_train[i][m][n+3]+x_train[i][m+1][n]+x_train[i][m+1][n+1]+x_train[i][m+1][n+2]+x_train[i][m+1][n+3]+x_train[i][m+2][n]+x_train[i][m+2][n+1]+x_train[i][m+2][n+2]+x_train[i][m+2][n+3]+x_train[i][m+3][n]+x_train[i][m+3][n+1]+x_train[i][m+3][n+2]+x_train[i][m+3][n+3])/16\n",
        "#             print ('n=',n)\n",
        "#             print ('j=',j)\n",
        "            j=j+1\n",
        "            \n",
        "            \n",
        "\n",
        "x_train1=x_train1.reshape(60000,49)\n",
        "\n",
        "i=0\n",
        "for i in range(10000):\n",
        "    j=0\n",
        "    m=0\n",
        "    n=0\n",
        "    for m in range (0,27,4):\n",
        "        n=0\n",
        "        for  n in range (0,27,4):\n",
        "            x_test1[i][j]=(x_test[i][m][n]+x_test[i][m][n+1]+x_test[i][m][n+2]+x_test[i][m][n+3]+x_test[i][m+1][n]+x_test[i][m+1][n+1]+x_test[i][m+1][n+2]+x_test[i][m+1][n+3]+x_test[i][m+2][n]+x_test[i][m+2][n+1]+x_test[i][m+2][n+2]+x_test[i][m+2][n+3]+x_test[i][m+3][n]+x_test[i][m+3][n+1]+x_test[i][m+3][n+2]+x_test[i][m+3][n+3])/16\n",
        "            j=j+1\n",
        "\n",
        "x_test1=x_test1.reshape(10000,49)\n",
        "x_train=x_train.reshape(60000,784)\n",
        "x_test=x_test.reshape(10000,784)\n",
        "print(\"finish\")\n",
        "# print(x_test1.shape)\n",
        "# print(x_train1.shape)\n",
        "# print(x_train1[0])\n",
        "# print (x_train[0])\n",
        "#------------------plot--------------------\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.subplot(321)\n",
        "# plt.imshow(x_train1[0],cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(322)\n",
        "# plt.imshow(x_train[0],cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(323)\n",
        "# plt.imshow(x_train1[6],cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(324)\n",
        "# plt.imshow(x_train[6],cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(325)\n",
        "# plt.imshow(x_train1[59999],cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(326)\n",
        "# plt.imshow(x_train[59999],cmap=plt.get_cmap('gray'))\n",
        "# plt.show()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PJqdqZAdQ5SE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "q_QVn7XtOGU6",
        "colab_type": "code",
        "outputId": "5331d117-ff80-4111-c9bc-ad52721e5c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "# -------------plot-----------------\n",
        "x_test1=x_test1.reshape(10000,7,7)\n",
        "x_test=x_test.reshape(10000,28,28)\n",
        "x_test1[0]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.subplot(121)\n",
        "plt.imshow(x_test[0],cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(122)\n",
        "plt.imshow(x_test1[0],cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAADqCAYAAADjwE/yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGMUXAADDKL4AABhG8QUAwDCKLwAAhv0Pmn85QiALCRwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uJndkJK1ovXh",
        "colab_type": "code",
        "outputId": "1ba842ff-fe85-4667-8a28-43d5fb52cd77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "x_train1[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.0120098 , 0.32132354,\n",
              "       0.48970592, 0.50122553, 0.4230392 , 0.        , 0.        ,\n",
              "       0.00441176, 0.4612745 , 0.52524507, 0.16789216, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.00269608, 0.48504901,\n",
              "       0.29142156, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.21102941, 0.82598042, 0.01617647, 0.        ,\n",
              "       0.        , 0.21936274, 0.65539217, 0.64166671, 0.1507353 ,\n",
              "       0.        , 0.        , 0.        , 0.21936277, 0.12132353,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "rXXv33fJQWIT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test1=x_test1.reshape(10000,49)\n",
        "x_train1=x_train1.reshape(60000,49)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cje-2eZ-4T4t",
        "colab_type": "code",
        "outputId": "486afb66-7a27-49b0-a2ec-958edf525367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# --------------tensorflow_hidden_layers*2---------------\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "#label one_hot\n",
        "a = np.array(y_train)\n",
        "n_class = a.max() + 1\n",
        "y_train1=np.eye(n_class)[a]\n",
        "\n",
        "a = np.array(y_test)\n",
        "n_class = a.max() + 1\n",
        "y_test1=np.eye(n_class)[a]\n",
        "\n",
        "# next_batch for new train_data\n",
        "def next_batch(num, data, labels):\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
        "\n",
        "#定义一些参数\n",
        "learning_rate = 0.005\n",
        "train_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "#定义3层感知机的神经单元个数\n",
        "n_input = 49\n",
        "n_hidden1 =20\n",
        "n_hidden2 = 20\n",
        "n_classes = 10\n",
        "\n",
        "#定义网络输入参数占位符\n",
        "x = tf.placeholder(tf.float32, shape=[None, n_input])\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
        "\n",
        "#定义权重与偏置\n",
        "weights = {'h1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "                  'h2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "                  'out': tf.Variable(tf.random_normal([n_hidden2, n_classes]))}\n",
        "\n",
        "biases = {'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "                'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "                'out': tf.Variable(tf.random_normal([n_classes]))}\n",
        "\n",
        "\n",
        "#定义推断过程\n",
        "def inference(input_x):\n",
        "    layer_1 = tf.nn.relu(tf.matmul(x, weights['h1']) + biases['b1'])\n",
        "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "#构建网络\n",
        "logits = inference(x)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "#定义损失函数与优化器\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "#定义评价指标(准确度)\n",
        "pre_correct = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(pre_correct, tf.float32))\n",
        "\n",
        "#初始化所有变量\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#strat time\n",
        "start_time=datetime.datetime.now()\n",
        "print('\\n Start training! \\n')\n",
        "\n",
        "#开始训练\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    total_batch = int(60000 / batch_size)\n",
        "\n",
        "    for epoch in range(train_epochs):\n",
        "        for batch in range(total_batch):\n",
        "            batch_x, batch_y = next_batch(batch_size,x_train1,y_train1)\n",
        "            sess.run(train_op, feed_dict={x:batch_x, y:batch_y})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            loss_, acc = sess.run([loss, accuracy], feed_dict={x:batch_x, y:batch_y})\n",
        "            print(\"epoch {},  loss {:.4f}, acc {:.3f}\".format(epoch, loss_, acc))\n",
        "\n",
        "    print(\"optimizer finished!\")\n",
        "    \n",
        "\n",
        "    #计算测试集的准确度 \n",
        "    test_acc = sess.run(accuracy, feed_dict={x:x_test1, y:y_test1})\n",
        "    print('test accuracy', test_acc)\n",
        "    \n",
        "    finish_time = datetime.datetime.now()\n",
        "    print ('\\n finish training! \\n')\n",
        "    print ('\\n the duration is : %s \\n'%(str(finish_time-start_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Start training! \n",
            "\n",
            "epoch 0,  loss 0.2890, acc 0.922\n",
            "optimizer finished!\n",
            "test accuracy 0.9396\n",
            "\n",
            " finish training! \n",
            "\n",
            "\n",
            " the duration is : 0:00:33.675913 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k2HAUUzPA6-r",
        "colab_type": "code",
        "outputId": "62a75227-cb92-454a-fc3a-317289fec136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# -------------------tensorflow_hidden_layers*1---------------------\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "#label one_hot\n",
        "a = np.array(y_train)\n",
        "n_class = a.max() + 1\n",
        "y_train1=np.eye(n_class)[a]\n",
        "\n",
        "a = np.array(y_test)\n",
        "n_class = a.max() + 1\n",
        "y_test1=np.eye(n_class)[a]\n",
        "\n",
        "# next_batch for new train_data\n",
        "def next_batch(num, data, labels):\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
        "\n",
        "#定义一些参数\n",
        "learning_rate = 0.005\n",
        "train_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "#定义3层感知机的神经单元个数\n",
        "n_input = 49\n",
        "n_hidden1 = 20\n",
        "# n_hidden2 = 100\n",
        "n_classes = 10\n",
        "\n",
        "#定义网络输入参数占位符\n",
        "x = tf.placeholder(tf.float32, shape=[None, n_input])\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
        "\n",
        "#定义权重与偏置\n",
        "weights = {'h1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "#                   'h2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "                  'out': tf.Variable(tf.random_normal([n_hidden1, n_classes]))}\n",
        "\n",
        "biases = {'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "#                 'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "                'out': tf.Variable(tf.random_normal([n_classes]))}\n",
        "\n",
        "\n",
        "#定义推断过程\n",
        "def inference(input_x):\n",
        "    layer_1 = tf.nn.relu(tf.matmul(x, weights['h1']) + biases['b1'])\n",
        "#     layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
        "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "#构建网络\n",
        "logits = inference(x)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "#定义损失函数与优化器\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "#定义评价指标(准确度)\n",
        "pre_correct = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(pre_correct, tf.float32))\n",
        "\n",
        "#初始化所有变量\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#strat time\n",
        "start_time=datetime.datetime.now()\n",
        "print('\\n Start training! \\n')\n",
        "\n",
        "#开始训练\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    total_batch = int(60000 / batch_size)\n",
        "\n",
        "    for epoch in range(train_epochs):\n",
        "        for batch in range(total_batch):\n",
        "            batch_x, batch_y = next_batch(batch_size,x_train1,y_train1)\n",
        "            sess.run(train_op, feed_dict={x:batch_x, y:batch_y})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            loss_, acc = sess.run([loss, accuracy], feed_dict={x:batch_x, y:batch_y})\n",
        "            print(\"epoch {},  loss {:.4f}, acc {:.3f}\".format(epoch, loss_, acc))\n",
        "\n",
        "    print(\"optimizer finished!\")\n",
        "    \n",
        "\n",
        "    #计算测试集的准确度 \n",
        "    test_acc = sess.run(accuracy, feed_dict={x:x_test1, y:y_test1})\n",
        "    print('test accuracy', test_acc)\n",
        "    \n",
        "    finish_time = datetime.datetime.now()\n",
        "    print ('\\n finish training! \\n')\n",
        "    print ('\\n the duration is : %s \\n'%(str(finish_time-start_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Start training! \n",
            "\n",
            "epoch 0,  loss 0.2716, acc 0.891\n",
            "optimizer finished!\n",
            "test accuracy 0.9366\n",
            "\n",
            " finish training! \n",
            "\n",
            "\n",
            " the duration is : 0:00:32.101555 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N1q2BHTlcs8r",
        "colab_type": "code",
        "outputId": "250881a8-8ed2-4500-fa64-302cfe5b5594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# -----tensorflow_hidden_layers*3--------------\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "#label one_hot\n",
        "a = np.array(y_train)\n",
        "n_class = a.max() + 1\n",
        "y_train1=np.eye(n_class)[a]\n",
        "\n",
        "a = np.array(y_test)\n",
        "n_class = a.max() + 1\n",
        "y_test1=np.eye(n_class)[a]\n",
        "\n",
        "# next_batch for new train_data\n",
        "def next_batch(num, data, labels):\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
        "\n",
        "#定义一些参数\n",
        "learning_rate = 0.005\n",
        "train_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "#定义3层感知机的神经单元个数\n",
        "n_input = 49\n",
        "n_hidden1 =400\n",
        "n_hidden2 = 400\n",
        "n_hidden3 = 400\n",
        "n_classes = 10\n",
        "\n",
        "#定义网络输入参数占位符\n",
        "x = tf.placeholder(tf.float32, shape=[None, n_input])\n",
        "y = tf.placeholder(tf.float32, shape=[None, n_classes])\n",
        "\n",
        "#定义权重与偏置\n",
        "weights = {'h1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "                  'h2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "                  'h3': tf.Variable(tf.random_normal([n_hidden2,n_hidden3])),\n",
        "                  'out': tf.Variable(tf.random_normal([n_hidden3, n_classes]))}\n",
        "\n",
        "biases = {'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "                'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "                'b3': tf.Variable(tf.random_normal([n_hidden3])),\n",
        "                'out': tf.Variable(tf.random_normal([n_classes]))}\n",
        "\n",
        "\n",
        "#定义推断过程\n",
        "def inference(input_x):\n",
        "    layer_1 = tf.nn.relu(tf.matmul(x, weights['h1']) + biases['b1'])\n",
        "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']) + biases['b2'])\n",
        "    layer_3 = tf.nn.relu(tf.matmul(layer_2, weights['h3']) + biases['b3'])\n",
        "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "#构建网络\n",
        "logits = inference(x)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "#定义损失函数与优化器\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "#定义评价指标(准确度)\n",
        "pre_correct = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(pre_correct, tf.float32))\n",
        "\n",
        "#初始化所有变量\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#strat time\n",
        "start_time=datetime.datetime.now()\n",
        "print('\\n Start training! \\n')\n",
        "\n",
        "#开始训练\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    total_batch = int(60000 / batch_size)\n",
        "\n",
        "    for epoch in range(train_epochs):\n",
        "        for batch in range(total_batch):\n",
        "            batch_x, batch_y = next_batch(batch_size,x_train1,y_train1)\n",
        "            sess.run(train_op, feed_dict={x:batch_x, y:batch_y})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            loss_, acc = sess.run([loss, accuracy], feed_dict={x:batch_x, y:batch_y})\n",
        "            print(\"epoch {},  loss {:.4f}, acc {:.3f}\".format(epoch, loss_, acc))\n",
        "\n",
        "    print(\"optimizer finished!\")\n",
        "    \n",
        "\n",
        "    #计算测试集的准确度 \n",
        "    test_acc = sess.run(accuracy, feed_dict={x:x_test1, y:y_test1})\n",
        "    print('test accuracy', test_acc)\n",
        "    \n",
        "    finish_time = datetime.datetime.now()\n",
        "    print ('\\n finish training! \\n')\n",
        "    print ('\\n the duration is : %s \\n'%(str(finish_time-start_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Start training! \n",
            "\n",
            "epoch 0,  loss 52.1454, acc 0.953\n",
            "optimizer finished!\n",
            "test accuracy 0.9467\n",
            "\n",
            " finish training! \n",
            "\n",
            "\n",
            " the duration is : 0:00:40.141192 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VIuonFvPrK1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a00d84a0-9112-4d0c-bccd-0a9dcc046aa4"
      },
      "cell_type": "code",
      "source": [
        "# --------------sklearn-----------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "#from keras.datasets import mnist\n",
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# MLPRegressor(activation='relu', alpha=1e-10, batch_size='auto', beta_1=0.9,\n",
        "#       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "#       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
        "#       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
        "#       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
        "#       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
        "#       warm_start=False)\n",
        "\n",
        "\n",
        "x_train1=x_train1.reshape(60000,49)\n",
        "x_test1=x_test1.reshape(10000,49)\n",
        "\n",
        "rgs1 = MLPRegressor(activation='relu',solver = 'lbfgs', alpha = 1e-10, hidden_layer_sizes = (80,80), random_state = 1, max_iter = 80, learning_rate='constant', learning_rate_init=0.001)\n",
        "#                    \n",
        "                    \n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "print('\\n Start Reading! \\n')\n",
        "\n",
        "rgs1.fit(x_train1, y_train)\n",
        "\n",
        "y1_ANN = rgs1.predict(x_test1)\n",
        "\n",
        "finish_time = datetime.datetime.now()\n",
        "print('\\n Finish Reading! \\n')\n",
        "print('Reading from %s to %s'%(start_time,finish_time))\n",
        "print('\\n The duration is: %s \\n'%(str(finish_time-start_time)))\n",
        "\n",
        "# y1_round=numpy.zeros(shape=(10000,))\n",
        "\n",
        "for n in range (10000):\n",
        "     if y1_ANN[n] > 9:\n",
        "          y1_ANN[n]=9\n",
        "          y1_round = np.round(y1_ANN)\n",
        "\n",
        "acc1 = 1 - np.round(sum(np.abs(y1_round - y_test)/len(y1_round)),4)\n",
        "print('The Accuracy is:', acc1)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Start Reading! \n",
            "\n",
            "\n",
            " Finish Reading! \n",
            "\n",
            "Reading from 2019-03-10 15:17:47.377623 to 2019-03-10 15:18:19.029328\n",
            "\n",
            " The duration is: 0:00:31.651705 \n",
            "\n",
            "The Accuracy is: 0.28090000000000004\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}